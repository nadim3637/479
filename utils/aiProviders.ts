import { AIModel } from "../types";

export const callProvider = async (model: AIModel, key: string, messages: any[], tools?: any) => {
    switch (model.provider) {
        case 'Groq':
            return callGroq(model.id, key, messages, tools);
        case 'Gemini':
            return callGemini(model.id, key, messages);
        case 'OpenAI':
            return callOpenAI(model.id, key, messages, tools);
        case 'Claude':
            return callClaude(model.id, key, messages);
        case 'DeepSeek':
            return callDeepSeek(model.id, key, messages);
        case 'Mistral':
            return callMistral(model.id, key, messages);
        case 'Ollama':
            return callOllama(model.id, messages);
        // Add others as needed
        default:
             // Fallback to OpenAI compatible endpoint for many others
            return callGenericOpenAICompatible(model.provider, model.id, key, messages);
    }
};

const callGroq = async (modelName: string, key: string, messages: any[], tools?: any) => {
    const url = "https://api.groq.com/openai/v1/chat/completions";
    const payload: any = {
        model: modelName,
        messages,
        temperature: 0.7
    };
    if (tools) payload.tools = tools;

    const res = await fetch(url, {
        method: "POST",
        headers: {
            "Authorization": `Bearer ${key}`,
            "Content-Type": "application/json"
        },
        body: JSON.stringify(payload)
    });

    if (!res.ok) {
        const txt = await res.text();
        throw new Error(`Groq Error ${res.status}: ${txt}`);
    }

    const data = await res.json();
    return data.choices[0].message.content;
};

const callGemini = async (modelName: string, key: string, messages: any[]) => {
    // Gemini API format is different (Google Generative AI)
    // Map messages to Gemini format
    // For simplicity, we can use the OpenAI compatible endpoint if available, or direct REST
    // Google Vertex AI / AI Studio
    
    // Using v1beta REST API for simplicity
    // https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent?key={key}
    
    // Simple message mapping
    const contents = messages.map((m: any) => ({
        role: m.role === 'user' ? 'user' : 'model',
        parts: [{ text: typeof m.content === 'string' ? m.content : JSON.stringify(m.content) }]
    }));
    
    // System instruction hack for Gemini (if first message is system)
    let systemInstruction = undefined;
    if (contents.length > 0 && messages[0].role === 'system') {
        systemInstruction = { parts: [{ text: messages[0].content }] };
        contents.shift();
    }

    const url = `https://generativelanguage.googleapis.com/v1beta/models/${modelName}:generateContent?key=${key}`;
    const res = await fetch(url, {
        method: "POST",
        headers: { "Content-Type": "application/json" },
        body: JSON.stringify({ contents, systemInstruction })
    });

    if (!res.ok) {
        const txt = await res.text();
        throw new Error(`Gemini Error ${res.status}: ${txt}`);
    }
    
    const data = await res.json();
    return data.candidates?.[0]?.content?.parts?.[0]?.text || "";
};

const callOpenAI = async (modelName: string, key: string, messages: any[], tools?: any) => {
    const url = "https://api.openai.com/v1/chat/completions";
    const payload: any = {
        model: modelName,
        messages,
        temperature: 0.7
    };
    if (tools) payload.tools = tools;

    const res = await fetch(url, {
        method: "POST",
        headers: {
            "Authorization": `Bearer ${key}`,
            "Content-Type": "application/json"
        },
        body: JSON.stringify(payload)
    });

    if (!res.ok) {
        const txt = await res.text();
        throw new Error(`OpenAI Error ${res.status}: ${txt}`);
    }
    const data = await res.json();
    return data.choices[0].message.content;
};

const callClaude = async (modelName: string, key: string, messages: any[]) => {
    // Anthropic API
    const url = "https://api.anthropic.com/v1/messages";
    
    // Filter system message
    let system = "";
    const cleanMessages = messages.filter((m: any) => {
        if (m.role === 'system') {
            system += m.content + "\n";
            return false;
        }
        return true;
    });

    const res = await fetch(url, {
        method: "POST",
        headers: {
            "x-api-key": key,
            "anthropic-version": "2023-06-01",
            "Content-Type": "application/json"
        },
        body: JSON.stringify({
            model: modelName,
            max_tokens: 4096,
            system: system,
            messages: cleanMessages
        })
    });

    if (!res.ok) {
        const txt = await res.text();
        throw new Error(`Claude Error ${res.status}: ${txt}`);
    }
    const data = await res.json();
    return data.content[0].text;
};

const callDeepSeek = async (modelName: string, key: string, messages: any[]) => {
    // DeepSeek is OpenAI compatible
    const url = "https://api.deepseek.com/chat/completions";
    const payload = { model: modelName, messages };
    
    const res = await fetch(url, {
        method: "POST",
        headers: {
             "Authorization": `Bearer ${key}`,
             "Content-Type": "application/json"
        },
        body: JSON.stringify(payload)
    });
     if (!res.ok) {
        const txt = await res.text();
        throw new Error(`DeepSeek Error ${res.status}: ${txt}`);
    }
    const data = await res.json();
    return data.choices[0].message.content;
};

const callMistral = async (modelName: string, key: string, messages: any[]) => {
    const url = "https://api.mistral.ai/v1/chat/completions";
    const payload = { model: modelName, messages };
    const res = await fetch(url, {
        method: "POST",
        headers: {
             "Authorization": `Bearer ${key}`,
             "Content-Type": "application/json"
        },
        body: JSON.stringify(payload)
    });
     if (!res.ok) {
        const txt = await res.text();
        throw new Error(`Mistral Error ${res.status}: ${txt}`);
    }
    const data = await res.json();
    return data.choices[0].message.content;
};

const callOllama = async (modelName: string, messages: any[]) => {
    const url = "http://localhost:11434/api/chat"; // Requires local access or bridge
    // If running in Vercel, this won't reach user's localhost.
    // BUT the prompt said "Phase 9 â€” Local AI ... Sab cloud AI mar bhi jaaye ... App phir bhi chalega"
    // This implies either the user is hosting this locally OR there's a tunnel.
    // I will implement the call assuming connectivity.
    
    const payload = { model: modelName, messages, stream: false };
    try {
        const res = await fetch(url, {
            method: "POST",
            headers: { "Content-Type": "application/json" },
            body: JSON.stringify(payload)
        });
        if (!res.ok) throw new Error("Ollama Error");
        const data = await res.json();
        return data.message.content;
    } catch (e: any) {
        throw new Error("Local Ollama unreachable: " + e.message);
    }
};

const callGenericOpenAICompatible = async (provider: string, modelName: string, key: string, messages: any[]) => {
    // OpenRouter, Together, etc usually follow OpenAI format
    let url = "";
    if (provider === 'OpenRouter') url = "https://openrouter.ai/api/v1/chat/completions";
    else if (provider === 'Together') url = "https://api.together.xyz/v1/chat/completions";
    else if (provider === 'Perplexity') url = "https://api.perplexity.ai/chat/completions";
    else if (provider === 'Fireworks') url = "https://api.fireworks.ai/inference/v1/chat/completions";
    else if (provider === 'Cohere') url = "https://api.cohere.ai/v1/chat"; // Not OpenAI compat
    else return `Provider ${provider} not fully implemented yet.`;

    const payload = { model: modelName, messages };
    const res = await fetch(url, {
        method: "POST",
        headers: {
             "Authorization": `Bearer ${key}`,
             "Content-Type": "application/json"
        },
        body: JSON.stringify(payload)
    });
     if (!res.ok) {
        const txt = await res.text();
        throw new Error(`${provider} Error ${res.status}: ${txt}`);
    }
    const data = await res.json();
    return data.choices?.[0]?.message?.content || JSON.stringify(data);
};
